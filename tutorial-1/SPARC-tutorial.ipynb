{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71fbbad7-cc5c-4fd5-bcc7-e5493398afcd",
   "metadata": {
    "tags": []
   },
   "source": [
    "Jupyter# **<center>SPARC FAIR Codeathon 2022</center>**\n",
    "<center>\n",
    "<a href=\"https://sparc.science\">\n",
    "<img src=\"https://sparc.science/_nuxt/img/logo-sparc-wave-primary.8ed83a5.svg\" alt=\"SPARC\" width=\"150\"/>\n",
    "</a>\n",
    "</center>\n",
    "<center>\n",
    "<a href=\"https://sparc.science/help/2022-sparc-fair-codeathon\">\n",
    "<img src=\"https://images.ctfassets.net/6bya4tyw8399/2qgsOmFnm7wYIfRrPrqbgx/ae3255858aa12bfcebb52e95c7cacffe/codeathon-graphic.png\" alt=\"FAIR\" width=\"75\">\n",
    "</a>\n",
    "</center>\n",
    "\n",
    "## <center>Tutorial 1: Mapping 2D data to a 3D organ scaffold</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdde64e8-4021-4a6b-bae7-b90b40239999",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Introduction**\n",
    "Welcome to the first of the Quilted Tutorials! We will be demonstrating different features from the [**SPARC**](https://sparc.science/) project. The goal will be to project the 2D locations of neurites in the rat stomach onto a 3D scaffold of the organ. The data points and the 3D scaffold will be pulled from **SPARC** datasets. This workflow could be applied to datasets for other organ systems. \n",
    "\n",
    "Because the data is [**FAIR**](https://www.nature.com/articles/sdata201618) we can easily re-use and combine three different datasets of the spatial distribution of afferent and efferent vagal neurons. \n",
    "\n",
    "Here is the workflow for this tutorial:\n",
    "\n",
    "![workflow](img/workflow.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed86df40-1ae5-4ad9-9b88-cd87333aff43",
   "metadata": {},
   "source": [
    "## **Installing the dependencies**\n",
    "This tutorial relies on several Python packages that have been developed as part of the **SPARC** project. We will be installing them in order to complete this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcb5a50-148a-42d4-a984-d96c866dacc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f332370-006a-4e4e-bf58-acda9f2d660b",
   "metadata": {},
   "source": [
    "### **Imports**\n",
    "Here we import all of the required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cccacb-ac11-4313-ab0d-48072a51fa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from stl import mesh as msh\n",
    "from tqdm import tqdm\n",
    "from zipfile import ZipFile\n",
    "from mpl_toolkits import mplot3d\n",
    "from ipywidgets import interact, Checkbox, fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a7271b-14cc-4f8e-8913-84b548c6a7ff",
   "metadata": {},
   "source": [
    "## **Retrieving the data**\n",
    "Now that all the dependencies have been installed we will retrieve the data directly from the [**SPARC**](https://sparc.science) project website using the Pennsieve API for the SPARC portal. \n",
    "\n",
    "We will be using the following three datasets:\n",
    " * [Vagal afferents associated with the myenteric plexus of the rat stomach](https://sparc.science/datasets/10?type=dataset&datasetDetailsTab=files)\n",
    " * [Vagal afferents within the longitudinal and circular muscle layers of the rat stomach](https://sparc.science/datasets/11?type=dataset&datasetDetailsTab=files)\n",
    " * [Vagal efferents associated with the myenteric plexus of the rat stomach](https://sparc.science/datasets/12?type=dataset&datasetDetailsTab=files)\n",
    " \n",
    "We'll first define a few helper functions to search, display and download datasets within this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcbe547-9de8-478a-b201-9f60a35c91f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_dataset(query, limit=5):\n",
    "    \"\"\" Searches the SPARC data portal for the given query\n",
    "        Inputs: \n",
    "        query -- string to search as a keyword in the dataset\n",
    "        limit -- integer limit for the number of results to return, defualt 5\n",
    "        \n",
    "        Outputs:\n",
    "        rst -- string of concatenated json tags for return results with the id, version, name and tags fields only for all returned results\n",
    "        \n",
    "    \"\"\"\n",
    "    url = \"https://api.pennsieve.io/discover/search/datasets?limit=\"+str(limit)+\"&offset=0&query=\"+query+\"&orderBy=relevance&orderDirection=desc\"\n",
    "    headers = {\"Accept\": \"application/json\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    rst = []\n",
    "    for r in response.json()['datasets']:\n",
    "        rst += [{'id':r['id'], 'version':r['version'], 'name':r['name'], 'tags':r['tags']}]\n",
    "    return rst\n",
    "\n",
    "def print_folder_structure(dataId, version, max_level=3): # taken from stackoverflow\n",
    "    \"\"\" Print the directory structure of a dataset to the console output. This assumes that it is saved in the root directory with default filename.\n",
    "    \n",
    "    Inputs: \n",
    "    dataId -- integer id of the result\n",
    "    version -- integer dataset version \n",
    "    max_level -- integer depth of directory structure to return, default 3\n",
    "    \n",
    "    Outputs: \n",
    "    None \n",
    "    \"\"\"\n",
    "    startpath = \"Pennsieve-dataset-\"+str(dataId)+\"-version-\"+str(version)\n",
    "    for root, dirs, files in os.walk(startpath):\n",
    "        level = root.replace(startpath, '').count(os.sep)\n",
    "        if level == max_level: break\n",
    "        indent = ' ' * 4 * (level)\n",
    "        print('{}{}/'.format(indent, os.path.basename(root)))\n",
    "        subindent = ' ' * 4 * (level + 1)\n",
    "        for f in files:\n",
    "            print('{}{}'.format(subindent, f))\n",
    "\n",
    "def get_dataset(dataId, version, dest_dir=\".\"):\n",
    "    \"\"\" Save a dataset from the SPARC data portal using the Pennsieve API.\n",
    "    \n",
    "    Inputs: \n",
    "    dataId -- integer id of the dataset\n",
    "    version -- integer dataset version \n",
    "    dest_dir -- string directory to save data set into. Default is root.\n",
    "    \n",
    "    Outputs: \n",
    "    None \n",
    "    \"\"\"\n",
    "    url = \"https://api.pennsieve.io/discover/datasets/\"+str(dataId)+\"/versions/\"+str(version)+\"/download?\"\n",
    "    # download dataset\n",
    "    response = requests.get(url, stream = True)\n",
    "    file_zip = \"data.zip\"\n",
    "    data_file = open(file_zip,\"wb\")\n",
    "    for chunk in tqdm(response.iter_content(chunk_size=1024)):\n",
    "        data_file.write(chunk)\n",
    "    data_file.close()\n",
    "    # unzip dataset\n",
    "    with ZipFile(file_zip, 'r') as obj:\n",
    "       obj.extractall()\n",
    "    # delete temporary zip file\n",
    "    os.remove(file_zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef9703f-5467-47ca-b435-a9f33230fbe6",
   "metadata": {},
   "source": [
    "The following line searches for the data sets we are interested in (keyword 'vagal') and prints out the search results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214ba031-d8a4-4bbf-85e2-23ae5bc45301",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_dataset('vagal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe71af9-4dfd-4ecf-af86-441d504474e3",
   "metadata": {},
   "source": [
    "Next we will download the datasets of interest, which have IDs 10, 11 and 12. \n",
    "We can do this using the Pennsieve API, but if you have downloaded the entire tutorial folder (not just the notebook) or if you are accessing it through Google Colab the relevant files as already available in the `res` directory. \n",
    "\n",
    "Only run the next code block if you do not already have the data sets available because it may take a while depending on your connection speed.\n",
    "\n",
    "#### ⚠️  **SPARC Guru tip**: \n",
    "You can also browse the [SPARC Data Portal](https://sparc.science/data?type=dataset) on your web browser and download datasets to the `res` directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28b4584-2345-494c-bc88-7f3741fc2587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first three datasets are of interest so download them\n",
    "get_dataset(dataId=10, version=3)\n",
    "get_dataset(dataId=11, version=3)\n",
    "get_dataset(dataId=12, version=3)\n",
    "\n",
    "# Exploring downloaded dataset. \n",
    "# We need the derivative analysis result in derivative folder (we know this because we have inspected the dataset documentation in the manifest and README files)\n",
    "print_folder_structure(dataId=10, version=3)\n",
    "print_folder_structure(dataId=11, version=3)\n",
    "print_folder_structure(dataId=12, version=3)\n",
    "\n",
    "# copy the required files to res folder for further utilisation\n",
    "!mkdir res\n",
    "!mv Pennsieve-dataset-10-version-3/files/derivative/IGLE_data.xlsx res\n",
    "!mv Pennsieve-dataset-11-version-3/files/derivative/IMA_analyzed_data.xlsx res\n",
    "!mv Pennsieve-dataset-12-version-3/files/derivative/Efferent_data.xlsx res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd24f2a-8f0f-4ad8-9b94-7a7d09a98fbe",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Loading the 2D data**\n",
    "In the 2D datasets that we are using, the distances are in percentages relative to an origin situated in the pyloric end of the stomach for the y-axis (left to right direction), and near the oesophagus for the z-axis (bottom to top direction). We are going to transform these coordinates to match those of the 3D stomach scaffold, in millimetres. For this, we are first going to define the boundaries in the z-axis and the y-axis. \n",
    "\n",
    "Here is a 2D representation of the data we are going to map to the 3D organ scaffold (retrieved from [1](https://sparc.science/datasets/10)).\n",
    "![2d](img/2d_data_viz.png)\n",
    "\n",
    "We will define a few helper functions to load the Excel files into the JupyterLab environment in the correct units:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aab3c46-8946-4e7b-b1ac-64d4ab546ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_position(percent, min_val, max_val):\n",
    "    \"\"\" Converts the position from percentage to distance.\n",
    "    \n",
    "    Inputs:\n",
    "    percent -- float, percentage value.\n",
    "    min_val -- float, minimum distance for conversion.\n",
    "    max_val -- float, maximum distance for conversion.\n",
    "    \n",
    "    Outputs:\n",
    "    converted_value -- float, converted value.\n",
    "    \n",
    "    \"\"\"\n",
    "    return percent / 100 * (max_val - min_val) + min_val \n",
    "\n",
    "def load_data(data_name, col_keeps, y_lims, z_lims):\n",
    "    \"\"\" Loads the data from an .xlsx file.\n",
    "    \n",
    "    Inputs:\n",
    "    data_name -- str, nane of the .xlsx file to read.\n",
    "    col_keeps -- dict{str:str}, dictionnary containing the names of the columns\n",
    "        to keep.\n",
    "    y_lims -- list[int], limits for the y direction to convert back to mm,\n",
    "            first element is the minimum and second is the maximum.\n",
    "    z_lims -- list[int], limits for the z direction to convert back to mm,\n",
    "        first element is the minimum and second is the maximum.\n",
    "    \n",
    "    Outputs:\n",
    "    df -- DataFrame, data frame containing the desired data.\n",
    "    \n",
    "    \"\"\"\n",
    "    df = pd.read_excel(data_name)\n",
    "    # remove unnecessary columns\n",
    "    for col in df.columns:\n",
    "        if col in col_keeps:\n",
    "            df.rename(columns = {col:col_keeps[col]}, inplace = True)\n",
    "        else:\n",
    "            df.drop(col, axis=1, inplace=True)\n",
    "    df['y'] = get_position(df['%y'], y_lims[0], y_lims[1])\n",
    "    df['z'] = get_position(df['%x'], z_lims[0], z_lims[1]) # x becomes z\n",
    "    df['-%y'] = 100 - df['%y']\n",
    "    # change the area to mm\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a489af2e-2fb7-4139-9696-c16732c898cb",
   "metadata": {},
   "source": [
    "We can now load the 2D spatial data (locations of the neurons) into DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868f7e0a-e0bb-484c-9916-1b629673a872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define y-axis and z-axis boundaries based on geometry of 3D stomach scaffold\n",
    "z_lims = [0, 36.7]\n",
    "y_lims = [24.6, 0]\n",
    "\n",
    "# fields to load from the Excel file\n",
    "col_keeps = {'%x (distance from pylorus side)':'%x', '%y (distance from bottom)':'%y',\n",
    "             'Average IGLE Area (um²)':'area', 'Area Of Innervation':'area', \n",
    "             'Neuron Area Of Innervation (um²) -Convex Hull':'area', 'V/D':'face',\n",
    "             'specimen anatomical location':'face'}\n",
    "df_igle = load_data('res/IGLE_data.xlsx', col_keeps, y_lims, z_lims)\n",
    "df_ima = load_data('res/IMA_analyzed_data.xlsx', col_keeps, y_lims, z_lims)\n",
    "df_efferent = load_data('res/Efferent_data.xlsx', col_keeps, y_lims, z_lims)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6c51a2-15a2-4ded-b67b-6c3d3a43ba57",
   "metadata": {},
   "source": [
    "### **Preparing the 2D data**\n",
    "Now that we have loaded the datasets into Python, we are going to prepare the data for mapping. For this, we are going to define a helper function. This will convert our data from the pandas DataFrame to a numpy array for easier manipulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f9e7bc-53f7-45f9-9bae-93faf6d05e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_2D_data(df):\n",
    "    \"\"\" Extracts required fields from the data frame to prepare for projection to 3D scaffold faces\n",
    "    \n",
    "    Inputs: \n",
    "    df -- pandas dataframe with at least, x, y, area and face fields\n",
    "    \n",
    "    Outputs:\n",
    "    data_2_dim -- Nx2 numpy array two-dimensional data coordinates for the neurons\n",
    "    data_intensity -- Nx1 numpy array for the area of innervation of each neuron to encode colour intensity\n",
    "    data_face -- the face ont which the neurons must be projected (D for dorsal, V for ventral)    \n",
    "    \"\"\"\n",
    "    print(df)\n",
    "    data_array = np.array(df[['y','z','area','face']])\n",
    "\n",
    "    # Convert input data to numpy coordinate array and intensity array.\n",
    "    data_2_dim = np.array(data_array[:, (0,1)])\n",
    "    data_intensity = data_array[:,2]\n",
    "    data_face = data_array[:, 3]\n",
    "    \n",
    "    return data_2_dim, data_intensity, data_face"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b91b7f2-5565-4964-a087-5ceb9fd9ec45",
   "metadata": {},
   "source": [
    "Now we will use the helper function to preprocess each dataset and extract the relevant fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2a7408-73b9-4254-8c38-d1c19b756316",
   "metadata": {},
   "outputs": [],
   "source": [
    "efferent_2D, efferent_intensity, efferent_face = convert_2D_data(df_efferent)\n",
    "igle_2D, igle_intensity, igle_face = convert_2D_data(df_igle)\n",
    "ima_2D, ima_intensity, ima_face = convert_2D_data(df_ima)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7726675-cad7-4337-a96d-e78ac1976170",
   "metadata": {},
   "source": [
    "### **Loading the 3D mesh**\n",
    "The 2D data is all set.  Time to focus on the 3D organ scaffold! Let's load the 3D stomach mesh provided in the `data` folder for this tutorial. \n",
    "\n",
    "#### ⚠️  **SPARC Guru tip**: \n",
    "However, it is possible to use the [**SPARC** Mapping Tools](https://docs.sparc.science/docs/map-core-scaffold-mapping-tools) to generate this mesh and convert it the STL file format. These tools are available as Python modules as well. We leave this task as an exercise for the reader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4472c84-cbee-40a7-b2d5-939ae0d1f50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the STL file\n",
    "stomach_mesh = msh.Mesh.from_file('res/stom_surf_mesh.stl')\n",
    "\n",
    "# adjust with coordinate axes (imported with signs reversed)\n",
    "stomach_mesh.x -= np.min(stomach_mesh.x.flatten())\n",
    "stomach_mesh.y -= np.min(stomach_mesh.y.flatten())\n",
    "stomach_mesh.z -= np.min(stomach_mesh.z.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4894c8bb-1768-48e3-bdfd-5a629c4be5f6",
   "metadata": {},
   "source": [
    "### **Mapping data**\n",
    "We are now ready to map the 2D points to the 3D mesh.\n",
    "The mathematics of projections are well understood, and more advanced methods to achieve this result are available in the [**SPARC** Mapping Tools](https://docs.sparc.science/docs/map-core-scaffold-mapping-tools). However, for this tutorial we will define the following simple helper function to project points perpendicular to their input 2D plane, onto the 3D stomach scaffold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c194b062-8e3d-4f28-96d2-bb823b9509b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_3D(input_pts, data_face, input_mesh):\n",
    "    \"\"\" Maps an Nx2 numpy array of points in the y-z plane to have an\n",
    "    x coordinate based on the nearest point in the y-z plane on the input mesh.\n",
    "    Inputs:\n",
    "    input_pts -- np.array(N, 2) with y coordinates in column 0, and\n",
    "        z coordinates in column 1.\n",
    "    data_face -- np.array(N,) of strings 'V' or 'D' indicating ventral or dorsal face respectively\n",
    "    input_mesh -- numpy-stl mesh object with the target mesh.\n",
    "    Outputs:\n",
    "    out: np.array(N, 3), numpy array with x coordinates in column 0,\n",
    "        y coordinates in column 1, and z coordinates in column 2.\n",
    "    \"\"\"\n",
    "    # Create list of vertices from the vectors of each triangle in the mesh.\n",
    "    vert = np.around(np.unique(input_mesh.vectors.reshape(\n",
    "        [int(input_mesh.vectors.size/3), 3]), axis=0),2)\n",
    "\n",
    "    # initial output array\n",
    "    out = np.zeros((input_pts.shape[0], 3))\n",
    "\n",
    "    # iterate over all points in y-z plane and find the closest mesh vertex in this plane\n",
    "    for i, pt in enumerate(input_pts):\n",
    "    \n",
    "        if data_face[i] == 'V' or data_face[i] == 'Stomach - ventral':\n",
    "            vert_candidate = vert[vert[:, 0] < 9.0, :]\n",
    "            offset = 1\n",
    "            jitter = 0.31\n",
    "        else:\n",
    "            vert_candidate = vert[vert[:, 0] > 8.7, :]\n",
    "            offset = -1\n",
    "            jitter = -0.31\n",
    "            \n",
    "        min_arg = np.argmin(np.sum(np.power(\n",
    "            np.abs((pt-vert_candidate[:, 1:3])),2), 1))\n",
    "        matched_x = vert_candidate[min_arg, 0]\n",
    "        \n",
    "        # Add some random movement so that triangles don't obscure points\n",
    "        matched_x -= offset + np.random.rand()*jitter\n",
    "        out[i, :] = [matched_x, pt[0], pt[1]]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a14fb5-3735-4aa5-9645-d567bbdc60ba",
   "metadata": {},
   "source": [
    "Now we will use the helper function to project points for each of the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1e8106-85c9-4376-883b-abff8ed01f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project to 3D stomach surface.\n",
    "efferent_3D = map_to_3D(efferent_2D, efferent_face, stomach_mesh)\n",
    "igle_3D = map_to_3D(igle_2D, igle_face, stomach_mesh)\n",
    "ima_3D = map_to_3D(ima_2D, ima_face, stomach_mesh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0aa87c1-6cfe-42d0-a880-ddce65a50821",
   "metadata": {},
   "source": [
    "### **Visualising data**\n",
    "The data has now been projected onto the 3D mesh. All that is left to do is visualise the mesh and the nerves! We are going to use the matplotlib package to visualise the data. However, there are **SPARC**-associated tools, such as [OpenCMISS](https://opencmiss.org) that provide a richer visualisation experience. \n",
    "\n",
    "#### ⚠️  **SPARC Guru tip**: \n",
    "Did you know that there are some **SPARC** tools that you can use to visualise the data? For this tutorial, we decided to keep everything within a Jupyter Lab environment. There are compatible Python APIs that could integrate the Scaffold Mapping Tool directly into this Jupyter Notebook but that process is more involved! Take a look [**SPARC** Mapping Tools](https://docs.sparc.science/docs/map-core-scaffold-mapping-tools) to read the documentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387d707a-0a7d-4d44-8163-0ab5b936de48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable interactivity in jupyterlab.\n",
    "%matplotlib widget \n",
    "\n",
    "def plotting_fct(mesh, efferent_3D, igle_3D, ima_3D, efferent_intensity, \n",
    "                 igle_intensity, ima_intensity, efferent, igle, ima):\n",
    "    # Start a matplotlib 3d interactive figure.\n",
    "    fig = plt.figure()\n",
    "    fig.suptitle('Projection visualisation', fontsize=16)\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "    # Add mesh as triangle polygons to 3d matplotlib view.\n",
    "    faces = mplot3d.art3d.Poly3DCollection(mesh.vectors, color=(0.960, 0.803, 0.650))\n",
    "    faces.set_edgecolor((0.960, 0.803, 0.650, 0.1))\n",
    "    faces.set_alpha(0.1)\n",
    "    ax.add_collection3d(faces)\n",
    "\n",
    "    # Plot the projected neurons coloured by their area of innervation.\n",
    "    if efferent:\n",
    "        ax.scatter(efferent_3D[:,0], efferent_3D[:,1], efferent_3D[:,2], \n",
    "                   c=efferent_intensity, cmap='Blues')\n",
    "        \n",
    "    if ima:\n",
    "        ax.scatter(ima_3D[:,0], ima_3D[:,1], ima_3D[:,2], \n",
    "                   c=ima_intensity, cmap='PuRd')\n",
    "        \n",
    "    if igle:\n",
    "        ax.scatter(igle_3D[:,0], igle_3D[:,1], igle_3D[:,2], \n",
    "                    c=igle_intensity, cmap='BuGn')        \n",
    "\n",
    "\n",
    "    # Scale view to the mesh size and turn off axis chrome.\n",
    "    ax.set_xlim(0,40)\n",
    "    ax.set_ylim(-10,30)\n",
    "    ax.set_zlim(-10,30)\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Show ventral surface and plot projected neurons.\n",
    "    ax.view_init(elev=10, azim=-57, vertical_axis='y')\n",
    "    plt.show()\n",
    "\n",
    "efferent=Checkbox(value=True, description=\"efferent\")\n",
    "igle=Checkbox(value=False, description=\"IGLE\")\n",
    "ima=Checkbox(value=False, description=\"IMA\")\n",
    "    \n",
    "interact(plotting_fct, mesh=fixed(stomach_mesh), efferent_3D=fixed(efferent_3D), \n",
    "            igle_3D=fixed(igle_3D), ima_3D=fixed(ima_3D),\n",
    "            efferent_intensity=fixed(efferent_intensity), \n",
    "            igle_intensity=fixed(igle_intensity), ima_intensity=fixed(ima_intensity), \n",
    "            efferent=efferent, igle=igle, ima=ima)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3e7ecb-389c-42ff-995e-ef700b6501a4",
   "metadata": {},
   "source": [
    "### **Congratulations**\n",
    "You have successfully completed your a Quilted Tutorial and are now on your way to becoming a **SPARC** Guru! \n",
    "\n",
    "We invite you to reuse this tutorial and explore the possibilities of using **SPARC** tools when possible. Try different things, such as adding ***your own code*** to generate a new stomach mesh with [Scaffold Maker](https://github.com/ABI-Software/scaffoldmaker). \n",
    "\n",
    "[1] T. L. Powley et al., “Spatial distribution and morphometric characterization of vagal afferents associated with the myenteric plexus of the rat stomach.” SPARC Consortium, 2019. doi: 10.26275/WZRY-SF7V."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
