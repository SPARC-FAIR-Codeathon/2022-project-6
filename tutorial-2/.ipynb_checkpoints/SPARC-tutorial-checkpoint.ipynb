{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71fbbad7-cc5c-4fd5-bcc7-e5493398afcd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **<center>SPARC FAIR Codeathon 2022</center>**\n",
    "<center>\n",
    "<a href=\"https://sparc.science\">\n",
    "<img src=\"https://sparc.science/_nuxt/img/logo-sparc-wave-primary.8ed83a5.svg\" alt=\"SPARC\" width=\"150\"/>\n",
    "</a>\n",
    "</center>\n",
    "<center>\n",
    "<a href=\"https://sparc.science/help/2022-sparc-fair-codeathon\">\n",
    "<img src=\"https://images.ctfassets.net/6bya4tyw8399/2qgsOmFnm7wYIfRrPrqbgx/ae3255858aa12bfcebb52e95c7cacffe/codeathon-graphic.png\" alt=\"FAIR\" width=\"75\">\n",
    "</a>\n",
    "</center>\n",
    "\n",
    "## <center>Tutorial 2: Resampling data for simulations</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdde64e8-4021-4a6b-bae7-b90b40239999",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Introduction**\n",
    "Welcome to the second of the Quilted Tutorials! We will be demonstrating different features from the [**SPARC**](https://sparc.science/) project. The goal will be to download some **SPARC** datasets and resample them so that they can be used for simulations. Because the data is [**FAIR**](https://www.nature.com/articles/sdata201618) we will be combining three different datasets of the spatial distribution of the vagal afferents and efferents. Here is the workflow for this tutorial\n",
    "\n",
    "![workflow](img/workflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed86df40-1ae5-4ad9-9b88-cd87333aff43",
   "metadata": {},
   "source": [
    "## **Installing the dependencies**\n",
    "This tutorial relies on several Python packages that have been developed as part of the **SPARC** project. We will be installing them in order to complete this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcb5a50-148a-42d4-a984-d96c866dacc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install openpyxl\n",
    "!pip install ipywidgets\n",
    "!pip install numpy\n",
    "!pip install numpy-stl\n",
    "!pip install matplotlib\n",
    "!pip install ipympl\n",
    "!pip install scipy\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f332370-006a-4e4e-bf58-acda9f2d660b",
   "metadata": {},
   "source": [
    "## **Imports**\n",
    "Here we import all of the dependencies that we will need to run the code correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cccacb-ac11-4313-ab0d-48072a51fa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from stl import mesh as msh\n",
    "from tqdm import tqdm\n",
    "from zipfile import ZipFile\n",
    "from mpl_toolkits import mplot3d\n",
    "from ipywidgets import interact, fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3172f15f-62d5-48c4-abf4-43fc28631e3e",
   "metadata": {},
   "source": [
    "## **Retrieving the data**\n",
    "Now that all the dependencies have been installed we will retrieve the data directly from the [**SPARC**](https://sparc.science) project website. \n",
    "We will be using the following three datasets:\n",
    " * [Vagal afferents associated with the myenteric plexus of the rat stomach](https://sparc.science/datasets/10?type=dataset&datasetDetailsTab=files)\n",
    " * [Vagal afferents within the longitudinal and circular muscle layers of the rat stomach](https://sparc.science/datasets/11?type=dataset&datasetDetailsTab=files)\n",
    " * [Vagal efferents associated with the myenteric plexus of the rat stomach](https://sparc.science/datasets/12?type=dataset&datasetDetailsTab=files)\n",
    " \n",
    "You can search through all of the **SPARC** datasets [here](https://sparc.science/data?type=dataset) or simply click on the links above to be directed to the datasets. \n",
    "\n",
    "It is possible to download the entire dataset by clicking on the purple ***Download full dataset*** button in the **Download Dataset** tab or to download specific files by selecting files and folders in the **Dataset Files** tab located at the bottom of the page. \n",
    "\n",
    "For this tutorial, we are only interested in the contents of the _derivative_ folder which contains two .xlsx files: one with the data (IGLE_data.xlsx, IMA_analyzed_data.xlsx, and Efferent_data.xlsx) and a manifest (manifest.xlsx). Enter the _derivative_ folder and select the .xlsx file containing the data by ticking the box in front of it. Download the file by clicking the **Download Selected Files and Folders** button at the bottom. You will then be prompted to select the location in which to save it. For each dataset, save it in the _SPARC-tutorial_ folder. \n",
    "\n",
    "#### ⚠️  **SPARC Guru tip**: \n",
    "Ever hear of Pennsieve? It's the **SPARC** tool to use if you want to avoid downloading the data manually! Check out the documentation for it and try it out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da1f905-549e-4e28-9985-049034780c91",
   "metadata": {},
   "source": [
    "### **Helper functions**\n",
    "Now that we have installed and imported the required dependencies, we are going to define some helper functions to retrieve the data.\n",
    "\n",
    "#### _search\\_dataset_\n",
    "This function will search the **SPARC** datasets for a given keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae30174e-18e7-4f4b-a448-bb5d308151c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_dataset(query, limit=5):\n",
    "    \"\"\" Searches the SPARC data portal for the given query\n",
    "    \n",
    "    Inputs: \n",
    "    query -- str, string to search as a keyword in the dataset\n",
    "    limit -- int, integer limit for the number of results to return, defualt 5\n",
    "        \n",
    "    Outputs:\n",
    "    rst -- str, string of concatenated json tags for return results with the id, version, name and tags fields only for all returned results\n",
    "        \n",
    "    \"\"\"\n",
    "    url = \"https://api.pennsieve.io/discover/search/datasets?limit=\"+str(limit)+\"&offset=0&query=\"+query+\"&orderBy=relevance&orderDirection=desc\"\n",
    "    headers = {\"Accept\": \"application/json\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    rst = []\n",
    "    for r in response.json()['datasets']:\n",
    "        rst += [{'id':r['id'], 'version':r['version'], 'name':r['name'], 'tags':r['tags']}]\n",
    "    return rst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d685643c-1ff5-4151-bbe0-6a6243c06442",
   "metadata": {},
   "source": [
    "#### _print\\_folder\\_structure_\n",
    "This function will prints the structure of the dataset directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819e4517-4480-480c-8249-397757171647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_folder_structure(dataId, version, max_level=3): # taken from stackoverflow\n",
    "    \"\"\" Print the directory structure of a dataset to the console output. \n",
    "    This assumes that it is saved in the root directory with default filename.\n",
    "    \n",
    "    Inputs: \n",
    "    dataId -- integer id of the result\n",
    "    version -- integer dataset version \n",
    "    max_level -- integer depth of directory structure to return, default 3\n",
    "    \n",
    "    Outputs: \n",
    "    None \n",
    "    \"\"\"\n",
    "    startpath = \"Pennsieve-dataset-\"+str(dataId)+\"-version-\"+str(version)\n",
    "    for root, dirs, files in os.walk(startpath):\n",
    "        level = root.replace(startpath, '').count(os.sep)\n",
    "        if level == max_level: break\n",
    "        indent = ' ' * 4 * (level)\n",
    "        print('{}{}/'.format(indent, os.path.basename(root)))\n",
    "        subindent = ' ' * 4 * (level + 1)\n",
    "        for f in files:\n",
    "            print('{}{}'.format(subindent, f))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4788d372-c166-4f1d-bb0a-9f28137b0252",
   "metadata": {},
   "source": [
    "#### _get\\_dataset_\n",
    "This function will retrieve the data from the **SPARC** portal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3145163f-aed8-4149-aaf9-5c273bafa12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(dataId, version, dest_dir=\".\"):\n",
    "    \"\"\" Save a dataset from the SPARC data portal using the Pennsieve API.\n",
    "    \n",
    "    Inputs: \n",
    "    dataId -- integer id of the dataset\n",
    "    version -- integer dataset version \n",
    "    dest_dir -- string directory to save data set into. Default is root.\n",
    "    \n",
    "    Outputs: \n",
    "    None \n",
    "    \"\"\"\n",
    "    url = \"https://api.pennsieve.io/discover/datasets/\"+str(dataId)+\"/versions/\"+str(version)+\"/download?\"\n",
    "    # download dataset\n",
    "    response = requests.get(url, stream = True)\n",
    "    file_zip = \"data.zip\"\n",
    "    data_file = open(file_zip,\"wb\")\n",
    "    for chunk in tqdm(response.iter_content(chunk_size=1024)):\n",
    "        data_file.write(chunk)\n",
    "    data_file.close()\n",
    "    # unzip dataset\n",
    "    with ZipFile(file_zip, 'r') as obj:\n",
    "       obj.extractall()\n",
    "    # delete temporary zip file\n",
    "    os.remove(file_zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82098264-5c53-4fdc-b72b-ebd2f29bb4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We are looking for vagal datasets so we create a query \"vagal\", returning 5 results using default value\n",
    "search_dataset('vagal')\n",
    "\n",
    "# The first three datasets are interesting so download them\n",
    "get_dataset(dataId=10, version=3)\n",
    "get_dataset(dataId=11, version=3)\n",
    "get_dataset(dataId=12, version=3)\n",
    "\n",
    "# Exploring downloaded dataset. \n",
    "# We need the derivative analysis result in derivative folder (we know this because we have inspected the dataset documentation in the manifest and README files)\n",
    "print_folder_structure(dataId=10, version=3) # Print this one as an example\n",
    "# print_folder_structure(dataId=11, version=3)\n",
    "# print_folder_structure(dataId=12, version=3)\n",
    "\n",
    "# copy the required files to res folder for further utilisation\n",
    "!mkdir res\n",
    "!mv Pennsieve-dataset-10-version-3/files/derivative/IGLE_data.xlsx res\n",
    "!mv Pennsieve-dataset-11-version-3/files/derivative/IMA_analyzed_data.xlsx res\n",
    "!mv Pennsieve-dataset-12-version-3/files/derivative/Efferent_data.xlsx res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826216f1-36e9-4b73-844c-14a80122ea46",
   "metadata": {},
   "source": [
    "## **Loading the 2D data**\n",
    "### **Helper functions**\n",
    "Now that we have retrieve the data, we are going to define some helper functions to load it.\n",
    "\n",
    "#### _get\\_position_\n",
    "This function will allow use to convert the position of the data from a percentage into a distance in mm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c787d47-d57e-485b-81d8-72595f951fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_position(percent, min_val, max_val):\n",
    "    \"\"\" Converts the position from percentage to distance.\n",
    "    \n",
    "    Inputs:\n",
    "    percent -- float, percentage value.\n",
    "    min_val -- float, minimum distance for conversion.\n",
    "    max_val -- float, maximum distance for conversion.\n",
    "    \n",
    "    Outputs:\n",
    "    converted_value -- float, converted value.\n",
    "    \n",
    "    \"\"\"\n",
    "    return percent / 100 * (max_val - min_val) + min_val "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59076ef0-df07-434f-a61d-cae3b5276ae3",
   "metadata": {},
   "source": [
    "#### _load\\_data_\n",
    "This function will allow use to extract the correct elements inside the data files and store them into a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d371b3-3046-4573-9423-cea7bebba4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_name, col_keeps, x_lims, y_lims):\n",
    "    \"\"\" Loads the data from an .xlsx file.\n",
    "    \n",
    "    Inputs:\n",
    "    data_name -- str, nane of the .xlsx file to read.\n",
    "    col_keeps -- dict{str:str}, dictionnary containing the names of the columns\n",
    "        to keep.\n",
    "    x_lims -- list[int], limits for the x direction to convert back to mm,\n",
    "            first element is the minimum and second is the maximum.\n",
    "    y_lims -- list[int], limits for the y direction to convert back to mm,\n",
    "        first element is the minimum and second is the maximum.\n",
    "    \n",
    "    Outputs:\n",
    "    df -- DataFrame, data frame containing the desired data.\n",
    "    \n",
    "    \"\"\"\n",
    "    df = pd.read_excel(data_name)\n",
    "    # remove unnecessary columns\n",
    "    for col in df.columns:\n",
    "        if col in col_keeps:\n",
    "            df.rename(columns = {col:col_keeps[col]}, inplace = True)\n",
    "        else:\n",
    "            df.drop(col, axis=1, inplace=True)\n",
    "    df['y'] = get_position(df['%y'], y_lims[0], y_lims[1])\n",
    "    df['x'] = get_position(df['%x'], x_lims[0], x_lims[1])\n",
    "    df['-%y'] = 100 - df['%y']\n",
    "    # change the area to mm\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197d2d6f-2137-4e64-a6a4-5ce14c9bc662",
   "metadata": {},
   "source": [
    "In the 2D datasets that we are using, the distances are in percentages relative to an origin situated in the pyloric end of the stomach for the y-axis and near the oesophagus for the z-axis. We are going to convert those into millimeters instead. For this, we are going to define the limits in the z- and y-axis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868f7e0a-e0bb-484c-9916-1b629673a872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup maximimum y and z widths based on scale in image.\n",
    "x_lims = [0, 36.7]\n",
    "y_lims = [24.6, 0]\n",
    "\n",
    "col_keeps = {'%x (distance from pylorus side)':'%x', '%y (distance from bottom)':'%y'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a489af2e-2fb7-4139-9696-c16732c898cb",
   "metadata": {},
   "source": [
    "We can now load the locations of the nerves into data frames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b000479-2aaf-49c6-8e1a-8915ff7a7f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "igle_df = load_data('res/IGLE_data.xlsx', col_keeps, x_lims, y_lims)\n",
    "ima_df = load_data('res/IMA_analyzed_data.xlsx', col_keeps, x_lims, y_lims)\n",
    "efferent_df = load_data('res/Efferent_data.xlsx', col_keeps, x_lims, y_lims) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3370e2ea-6a63-4811-93cb-14cc9bf9bb7a",
   "metadata": {},
   "source": [
    "## **Preparing the 2D data**\n",
    "### **Helper functions**\n",
    "Now that we have loaded the data, we are going to define some helper functions to process it.\n",
    "\n",
    "#### _prepare\\_data_\n",
    "This function will prepare the data to be plotted by resampling the data points and extract the density of nerves in 2D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf1daff-8511-4a6b-8c8f-6f7bcc5808b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df):\n",
    "    \"\"\" Prepares the data to be plotted by creating the probablity estimates and the sampled points.\n",
    "    \n",
    "    Inputs:\n",
    "    df -- \n",
    "    Outputs:\n",
    "    \n",
    "    \"\"\"\n",
    "    data_array = df\n",
    "    data_array = data_array[~data_array.isin([np.nan]).any(1)]\n",
    "\n",
    "    # Extract x and y\n",
    "    x = np.array(data_array['x'])\n",
    "    y = np.array(data_array['y'])\n",
    "\n",
    "    # Create meshgrid\n",
    "    xx, yy = np.mgrid[x_lims[0]:x_lims[1]:100j, y_lims[0]:y_lims[1]:100j]\n",
    "\n",
    "    positions = np.vstack([xx.ravel(), yy.ravel()])\n",
    "    values = np.vstack([x, y])\n",
    "    kernel = st.gaussian_kde(values)\n",
    "    prob_estimate = np.reshape(kernel(positions).T, xx.shape)\n",
    "\n",
    "    sampled_pts = kernel.resample(1000).T\n",
    "    \n",
    "    return xx, yy, prob_estimate, sampled_pts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5ba377-6762-42fb-a7d2-24fa9aa82319",
   "metadata": {},
   "source": [
    "Now that we have loaded are datasets into Python, we are going to prepare the data for plotting. For this, we are going to use our _prepare\\_data_ helper function. This will resample your data points using the desires probability distribution and provide us with the density of points in space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2a7408-73b9-4254-8c38-d1c19b756316",
   "metadata": {},
   "outputs": [],
   "source": [
    "efferent_xx, efferent_yy, efferent_est, efferent_pts = prepare_data(efferent_df)\n",
    "ima_xx, ima_yy, ima_est, ima_pts = prepare_data(ima_df)\n",
    "igle_xx, igle_yy, igle_est, igle_pts = prepare_data(igle_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0aa87c1-6cfe-42d0-a880-ddce65a50821",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Visualising data**\n",
    "We are now going to visualise the data in 2D. In the plot, the green point represent the resampled point and the color represents the density of the data points. Switch between nerve types by using the dropdown menu and see how the density changes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387d707a-0a7d-4d44-8163-0ab5b936de48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable interactivity in jupyterlab.\n",
    "%matplotlib widget \n",
    "\n",
    "def plotting_fct(efferent_xx, efferent_yy, efferent_est, efferent_pts,\n",
    "                 ima_xx, ima_yy, ima_est, ima_pts,\n",
    "                 igle_xx, igle_yy, igle_est, igle_pts,\n",
    "                 sel):\n",
    "    \n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    ax = fig.gca()\n",
    "\n",
    "    ax.set_xlim(x_lims[0], x_lims[1])\n",
    "    ax.set_ylim(y_lims[1], y_lims[0])\n",
    "\n",
    "    if sel == 'Efferent':\n",
    "        cfset = ax.contourf(efferent_xx, efferent_yy, efferent_est, levels=1000,cmap='coolwarm')\n",
    "        ax.imshow(np.rot90(efferent_est), cmap='coolwarm', extent=[x_lims[0], x_lims[1], y_lims[0], y_lims[1]])\n",
    "        ax.scatter(efferent_pts[:, 0], efferent_pts[:, 1], s=5, color='g')\n",
    "\n",
    "    if sel == 'IMA':\n",
    "        cfset = ax.contourf(ima_xx, ima_yy, ima_est, levels=1000,cmap='coolwarm')\n",
    "        ax.imshow(np.rot90(ima_est), cmap='coolwarm', extent=[x_lims[0], x_lims[1], y_lims[0], y_lims[1]])        \n",
    "        ax.scatter(ima_pts[:, 0], ima_pts[:, 1], s=5, color='g')\n",
    "\n",
    "    if sel == 'IGLE':\n",
    "        cfset = ax.contourf(igle_xx, igle_yy, igle_est, levels=1000,cmap='coolwarm')\n",
    "        ax.imshow(np.rot90(igle_est), cmap='coolwarm', extent=[x_lims[0], x_lims[1], y_lims[0], y_lims[1]])        \n",
    "        ax.scatter(igle_pts[:, 0], igle_pts[:, 1], s=5, color='g')\n",
    "\n",
    "    ax.set_xlabel('X (mm)')\n",
    "    ax.set_ylabel('Y (mm)')\n",
    "    plt.title('Gaussian Kernel density estimation')\n",
    "    plt.show()\n",
    "\n",
    "def onToggle(btn):\n",
    "    plotting_fct(efferent_xx=efferent_xx, efferent_yy=efferent_yy, efferent_est=efferent_est, efferent_pts=efferent_pts, \n",
    "         ima_xx=ima_xx, ima_yy=ima_yy, ima_est=ima_est, ima_pts=ima_pts, \n",
    "         igle_xx=igle_xx, igle_yy=igle_yy, igle_est=igle_est, igle_pts=igle_pts, sel=btn.owner.value)\n",
    "\n",
    "interact(plotting_fct, efferent_xx=fixed(efferent_xx), efferent_yy=fixed(efferent_yy), efferent_est=fixed(efferent_est), efferent_pts=fixed(efferent_pts), \n",
    "         ima_xx=fixed(ima_xx), ima_yy=fixed(ima_yy), ima_est=fixed(ima_est), ima_pts=fixed(ima_pts), \n",
    "         igle_xx=fixed(igle_xx), igle_yy=fixed(igle_yy), igle_est=fixed(igle_est), igle_pts=fixed(igle_pts), \n",
    "         sel=['Efferent', 'IGLE', 'IMA'])    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3e7ecb-389c-42ff-995e-ef700b6501a4",
   "metadata": {},
   "source": [
    "## **Congratulations**\n",
    "You have successfully completed the second Quilted Tutorial and are now well on your way to becoming a **SPARC** Guru! \n",
    "\n",
    "We invite you to reuse this tutorial and explore the possibilities of using **SPARC** tools when possible or using a different sampling kernel. You can use the resampled data to make simulations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
