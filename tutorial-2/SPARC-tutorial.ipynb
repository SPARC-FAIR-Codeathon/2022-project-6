{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71fbbad7-cc5c-4fd5-bcc7-e5493398afcd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **<center>SPARC FAIR Codeathon 2022</center>**\n",
    "<center>\n",
    "<a href=\"https://sparc.science\">\n",
    "<img src=\"https://sparc.science/_nuxt/img/logo-sparc-wave-primary.8ed83a5.svg\" alt=\"SPARC\" width=\"150\"/>\n",
    "</a>\n",
    "</center>\n",
    "<center>\n",
    "<a href=\"https://sparc.science/help/2022-sparc-fair-codeathon\">\n",
    "<img src=\"https://images.ctfassets.net/6bya4tyw8399/2qgsOmFnm7wYIfRrPrqbgx/ae3255858aa12bfcebb52e95c7cacffe/codeathon-graphic.png\" alt=\"FAIR\" width=\"75\">\n",
    "</a>\n",
    "</center>\n",
    "\n",
    "## <center>Tutorial 2: Resampling data for simulations</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdde64e8-4021-4a6b-bae7-b90b40239999",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Introduction**\n",
    "Welcome to the second of the Quilted Tutorials! We will be demonstrating different features from the [**SPARC**](https://sparc.science/) project. The goal will be to download some **SPARC** datasets and resample them so that they can be used for simulations. \n",
    "\n",
    "Because the data is [**FAIR**](https://www.nature.com/articles/sdata201618) we can easily re-use and combine three different datasets of the spatial distribution of afferent and efferent vagal neurons. Here is the workflow for this tutorial\n",
    "\n",
    "Here is the workflow for this tutorial:\n",
    "\n",
    "![workflow](img/workflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed86df40-1ae5-4ad9-9b88-cd87333aff43",
   "metadata": {},
   "source": [
    "## **Installing the dependencies**\n",
    "This tutorial relies on several Python packages that have been developed as part of the **SPARC** project. We will be installing them in order to complete this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcb5a50-148a-42d4-a984-d96c866dacc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f332370-006a-4e4e-bf58-acda9f2d660b",
   "metadata": {},
   "source": [
    "## **Imports**\n",
    "Here we import all of the dependencies that we will need to run the code correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cccacb-ac11-4313-ab0d-48072a51fa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from stl import mesh as msh\n",
    "from tqdm import tqdm\n",
    "from zipfile import ZipFile\n",
    "from mpl_toolkits import mplot3d\n",
    "from ipywidgets import interact, fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c81f0a-b9f5-41f0-b401-6117d97bc25b",
   "metadata": {},
   "source": [
    "## **Retrieving the data**\n",
    "Now that all the dependencies have been installed we will retrieve the data directly from the [**SPARC**](https://sparc.science) project website using the Pennsieve API for the SPARC portal. \n",
    "\n",
    "We will be using the following three datasets:\n",
    " * [Vagal afferents associated with the myenteric plexus of the rat stomach](https://sparc.science/datasets/10?type=dataset&datasetDetailsTab=files)\n",
    " * [Vagal afferents within the longitudinal and circular muscle layers of the rat stomach](https://sparc.science/datasets/11?type=dataset&datasetDetailsTab=files)\n",
    " * [Vagal efferents associated with the myenteric plexus of the rat stomach](https://sparc.science/datasets/12?type=dataset&datasetDetailsTab=files)\n",
    " \n",
    "We'll first define a few helper functions to search, display and download datasets within this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f44249-3c2e-4ad9-a123-64e81a5c88aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_dataset(query, limit=5):\n",
    "    \"\"\" Searches the SPARC data portal for the given query\n",
    "        Inputs: \n",
    "        query -- string to search as a keyword in the dataset\n",
    "        limit -- integer limit for the number of results to return, defualt 5\n",
    "        \n",
    "        Outputs:\n",
    "        rst -- string of concatenated json tags for return results with the id, version, name and tags fields only for all returned results\n",
    "        \n",
    "    \"\"\"\n",
    "    url = \"https://api.pennsieve.io/discover/search/datasets?limit=\"+str(limit)+\"&offset=0&query=\"+query+\"&orderBy=relevance&orderDirection=desc\"\n",
    "    headers = {\"Accept\": \"application/json\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    rst = []\n",
    "    for r in response.json()['datasets']:\n",
    "        rst += [{'id':r['id'], 'version':r['version'], 'name':r['name'], 'tags':r['tags']}]\n",
    "    return rst\n",
    "\n",
    "def print_folder_structure(dataId, version, max_level=3): # taken from stackoverflow\n",
    "    \"\"\" Print the directory structure of a dataset to the console output. This assumes that it is saved in the root directory with default filename.\n",
    "    \n",
    "    Inputs: \n",
    "    dataId -- integer id of the result\n",
    "    version -- integer dataset version \n",
    "    max_level -- integer depth of directory structure to return, default 3\n",
    "    \n",
    "    Outputs: \n",
    "    None \n",
    "    \"\"\"\n",
    "    startpath = \"Pennsieve-dataset-\"+str(dataId)+\"-version-\"+str(version)\n",
    "    for root, dirs, files in os.walk(startpath):\n",
    "        level = root.replace(startpath, '').count(os.sep)\n",
    "        if level == max_level: break\n",
    "        indent = ' ' * 4 * (level)\n",
    "        print('{}{}/'.format(indent, os.path.basename(root)))\n",
    "        subindent = ' ' * 4 * (level + 1)\n",
    "        for f in files:\n",
    "            print('{}{}'.format(subindent, f))\n",
    "\n",
    "def get_dataset(dataId, version, dest_dir=\".\"):\n",
    "    \"\"\" Save a dataset from the SPARC data portal using the Pennsieve API.\n",
    "    \n",
    "    Inputs: \n",
    "    dataId -- integer id of the dataset\n",
    "    version -- integer dataset version \n",
    "    dest_dir -- string directory to save data set into. Default is root.\n",
    "    \n",
    "    Outputs: \n",
    "    None \n",
    "    \"\"\"\n",
    "    url = \"https://api.pennsieve.io/discover/datasets/\"+str(dataId)+\"/versions/\"+str(version)+\"/download?\"\n",
    "    # download dataset\n",
    "    response = requests.get(url, stream = True)\n",
    "    file_zip = \"data.zip\"\n",
    "    data_file = open(file_zip,\"wb\")\n",
    "    for chunk in tqdm(response.iter_content(chunk_size=1024)):\n",
    "        data_file.write(chunk)\n",
    "    data_file.close()\n",
    "    # unzip dataset\n",
    "    with ZipFile(file_zip, 'r') as obj:\n",
    "       obj.extractall()\n",
    "    # delete temporary zip file\n",
    "    os.remove(file_zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255a7d06-221d-48fb-b0e6-aa4df4d90884",
   "metadata": {},
   "source": [
    "The following line searches for the data sets we are interested in (keyword 'vagal') and prints out the search results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38d5ac4-2e43-49f1-94dd-9a3446709a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_dataset('vagal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889fb3f1-a1c8-4c8b-a674-8061786c54d7",
   "metadata": {},
   "source": [
    "Next we will download the datasets of interest, which have IDs 10, 11 and 12. \n",
    "We can do this using the Pennsieve API, but if you have downloaded the entire tutorial folder (not just the notebook) or if you are accessing it through Google Colab the relevant files as already available in the `res` directory. \n",
    "\n",
    "Only run the next code block if you do not already have the data sets available because it may take a while depending on your connection speed.\n",
    "\n",
    "#### ⚠️  **SPARC Guru tip**: \n",
    "You can also browse the [SPARC Data Portal](https://sparc.science/data?type=dataset) on your web browser and download datasets to the `res` directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b553b0d-53c7-41a5-9199-d1b064892ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first three datasets are of interest so download them\n",
    "get_dataset(dataId=10, version=3)\n",
    "get_dataset(dataId=11, version=3)\n",
    "get_dataset(dataId=12, version=3)\n",
    "\n",
    "# Exploring downloaded dataset. \n",
    "# We need the derivative analysis result in derivative folder (we know this because we have inspected the dataset documentation in the manifest and README files)\n",
    "print_folder_structure(dataId=10, version=3)\n",
    "print_folder_structure(dataId=11, version=3)\n",
    "print_folder_structure(dataId=12, version=3)\n",
    "\n",
    "# copy the required files to res folder for further utilisation\n",
    "!mkdir res\n",
    "!mv Pennsieve-dataset-10-version-3/files/derivative/IGLE_data.xlsx res\n",
    "!mv Pennsieve-dataset-11-version-3/files/derivative/IMA_analyzed_data.xlsx res\n",
    "!mv Pennsieve-dataset-12-version-3/files/derivative/Efferent_data.xlsx res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826216f1-36e9-4b73-844c-14a80122ea46",
   "metadata": {},
   "source": [
    "### **Loading the 2D data**\n",
    "In the 2D datasets that we are using, the distances are in percentages relative to an origin situated in the pyloric end of the stomach for the y-axis (left to right direction), and near the oesophagus for the z-axis (bottom to top direction). We are going to transform these proportional measurements to millimetres. For this, we will first going to define the boundaries in the z-axis and the y-axis. \n",
    "\n",
    "Here is a 2D representation of the data we are going to visualise and resample (retrieved from [1](https://sparc.science/datasets/10)).\n",
    "![2d](img/2d_data_viz.png)\n",
    "\n",
    "We will define a few helper functions to load the Excel files into the JupyterLab environment in the correct units:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c787d47-d57e-485b-81d8-72595f951fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_position(percent, min_val, max_val):\n",
    "    \"\"\" Converts the position from percentage to distance.\n",
    "    \n",
    "    Inputs:\n",
    "    percent -- float, percentage value.\n",
    "    min_val -- float, minimum distance for conversion.\n",
    "    max_val -- float, maximum distance for conversion.\n",
    "    \n",
    "    Outputs:\n",
    "    converted_value -- float, converted value.\n",
    "    \n",
    "    \"\"\"\n",
    "    return percent / 100 * (max_val - min_val) + min_val \n",
    "\n",
    "def load_data(data_name, col_keeps, x_lims, y_lims):\n",
    "    \"\"\" Loads the data from an .xlsx file.\n",
    "    \n",
    "    Inputs:\n",
    "    data_name -- str, nane of the .xlsx file to read.\n",
    "    col_keeps -- dict{str:str}, dictionary containing the names of the columns\n",
    "        to keep.\n",
    "    x_lims -- list[int], limits for the x direction to convert back to mm,\n",
    "            first element is the minimum and second is the maximum.\n",
    "    y_lims -- list[int], limits for the y direction to convert back to mm,\n",
    "        first element is the minimum and second is the maximum.\n",
    "    \n",
    "    Outputs:\n",
    "    df -- DataFrame, data frame containing the desired data.\n",
    "    \n",
    "    \"\"\"\n",
    "    df = pd.read_excel(data_name)\n",
    "    # remove unnecessary columns\n",
    "    for col in df.columns:\n",
    "        if col in col_keeps:\n",
    "            df.rename(columns = {col:col_keeps[col]}, inplace = True)\n",
    "        else:\n",
    "            df.drop(col, axis=1, inplace=True)\n",
    "    df['y'] = get_position(df['%y'], y_lims[0], y_lims[1])\n",
    "    df['x'] = get_position(df['%x'], x_lims[0], x_lims[1])\n",
    "    df['-%y'] = 100 - df['%y']\n",
    "    # change the area to mm\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197d2d6f-2137-4e64-a6a4-5ce14c9bc662",
   "metadata": {},
   "source": [
    "In the 2D datasets that we are using, the distances are in percentages relative to an origin situated in the pyloric end of the stomach for the y-axis and near the oesophagus for the z-axis. We are going to convert those into millimetres instead. For this, we are going to define the limits in the z- and y-axis. We will also define the columns we want to keep in the loaded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868f7e0a-e0bb-484c-9916-1b629673a872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup maximimum y and z widths based on scale in image.\n",
    "x_lims = [0, 36.7]\n",
    "y_lims = [24.6, 0]\n",
    "\n",
    "col_keeps = {'%x (distance from pylorus side)':'%x', '%y (distance from bottom)':'%y'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a489af2e-2fb7-4139-9696-c16732c898cb",
   "metadata": {},
   "source": [
    "We can now load the locations of the nerves into data frames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b000479-2aaf-49c6-8e1a-8915ff7a7f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "igle_df = load_data('res/IGLE_data.xlsx', col_keeps, x_lims, y_lims)\n",
    "ima_df = load_data('res/IMA_analyzed_data.xlsx', col_keeps, x_lims, y_lims)\n",
    "efferent_df = load_data('res/Efferent_data.xlsx', col_keeps, x_lims, y_lims) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3370e2ea-6a63-4811-93cb-14cc9bf9bb7a",
   "metadata": {},
   "source": [
    "## **Processing the 2D data**\n",
    "Now that we have loaded the data, we will define a processing function.\n",
    "This function\n",
    "- estimates the probability density using a gaussian kernel method\n",
    "- then resamples 1000 points in 2D using this the esitmated probability densities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf1daff-8511-4a6b-8c8f-6f7bcc5808b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(df):\n",
    "    \"\"\" Prepares the data to be plotted by creating the probablity estimates and the sampled points.\n",
    "    \n",
    "    Inputs:\n",
    "    df -- \n",
    "    Outputs:\n",
    "    \n",
    "    \"\"\"\n",
    "    data_array = df\n",
    "    data_array = data_array[~data_array.isin([np.nan]).any(1)]\n",
    "\n",
    "    # Extract x and y\n",
    "    x = np.array(data_array['x'])\n",
    "    y = np.array(data_array['y'])\n",
    "\n",
    "    # Create meshgrid\n",
    "    xx, yy = np.mgrid[x_lims[0]:x_lims[1]:100j, y_lims[0]:y_lims[1]:100j]\n",
    "\n",
    "    positions = np.vstack([xx.ravel(), yy.ravel()])\n",
    "    values = np.vstack([x, y])\n",
    "    kernel = st.gaussian_kde(values)\n",
    "    prob_estimate = np.reshape(kernel(positions).T, xx.shape)\n",
    "\n",
    "    sampled_pts = kernel.resample(1000).T\n",
    "    \n",
    "    return xx, yy, prob_estimate, sampled_pts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5ba377-6762-42fb-a7d2-24fa9aa82319",
   "metadata": {},
   "source": [
    "Now that we have loaded are datasets into Python, we are going to prepare the data for plotting. For this, we are going to use our _prepare\\_data_ helper function. This will resample your data points using the desired probability distribution and provide us with the density of points in space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2a7408-73b9-4254-8c38-d1c19b756316",
   "metadata": {},
   "outputs": [],
   "source": [
    "efferent_xx, efferent_yy, efferent_est, efferent_pts = process_data(efferent_df)\n",
    "ima_xx, ima_yy, ima_est, ima_pts = process_data(ima_df)\n",
    "igle_xx, igle_yy, igle_est, igle_pts = process_data(igle_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0aa87c1-6cfe-42d0-a880-ddce65a50821",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Visualising data**\n",
    "We are now going to visualise the data in 2D. In the plot, the green point represent the resampled point and the color represents the density of the data points. Switch between neuron types by using the dropdown menu and see how the density changes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387d707a-0a7d-4d44-8163-0ab5b936de48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable interactivity in jupyterlab.\n",
    "%matplotlib widget \n",
    "\n",
    "def plotting_fct(efferent_xx, efferent_yy, efferent_est, efferent_pts,\n",
    "                 ima_xx, ima_yy, ima_est, ima_pts,\n",
    "                 igle_xx, igle_yy, igle_est, igle_pts,\n",
    "                 sel):\n",
    "    \n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    ax = fig.gca()\n",
    "\n",
    "    ax.set_xlim(x_lims[0], x_lims[1])\n",
    "    ax.set_ylim(y_lims[1], y_lims[0])\n",
    "\n",
    "    if sel == 'Efferent':\n",
    "        cfset = ax.contourf(efferent_xx, efferent_yy, efferent_est, levels=1000,cmap='coolwarm')\n",
    "        ax.imshow(np.rot90(efferent_est), cmap='coolwarm', extent=[x_lims[0], x_lims[1], y_lims[0], y_lims[1]])\n",
    "        ax.scatter(efferent_pts[:, 0], efferent_pts[:, 1], s=5, color='g')\n",
    "\n",
    "    if sel == 'IMA':\n",
    "        cfset = ax.contourf(ima_xx, ima_yy, ima_est, levels=1000,cmap='coolwarm')\n",
    "        ax.imshow(np.rot90(ima_est), cmap='coolwarm', extent=[x_lims[0], x_lims[1], y_lims[0], y_lims[1]])        \n",
    "        ax.scatter(ima_pts[:, 0], ima_pts[:, 1], s=5, color='g')\n",
    "\n",
    "    if sel == 'IGLE':\n",
    "        cfset = ax.contourf(igle_xx, igle_yy, igle_est, levels=1000,cmap='coolwarm')\n",
    "        ax.imshow(np.rot90(igle_est), cmap='coolwarm', extent=[x_lims[0], x_lims[1], y_lims[0], y_lims[1]])        \n",
    "        ax.scatter(igle_pts[:, 0], igle_pts[:, 1], s=5, color='g')\n",
    "\n",
    "    ax.set_xlabel('X (mm)')\n",
    "    ax.set_ylabel('Y (mm)')\n",
    "    plt.title('Gaussian Kernel density estimation')\n",
    "    plt.show()\n",
    "\n",
    "def onToggle(btn):\n",
    "    plotting_fct(efferent_xx=efferent_xx, efferent_yy=efferent_yy, efferent_est=efferent_est, efferent_pts=efferent_pts, \n",
    "         ima_xx=ima_xx, ima_yy=ima_yy, ima_est=ima_est, ima_pts=ima_pts, \n",
    "         igle_xx=igle_xx, igle_yy=igle_yy, igle_est=igle_est, igle_pts=igle_pts, sel=btn.owner.value)\n",
    "\n",
    "interact(plotting_fct, efferent_xx=fixed(efferent_xx), efferent_yy=fixed(efferent_yy), efferent_est=fixed(efferent_est), efferent_pts=fixed(efferent_pts), \n",
    "         ima_xx=fixed(ima_xx), ima_yy=fixed(ima_yy), ima_est=fixed(ima_est), ima_pts=fixed(ima_pts), \n",
    "         igle_xx=fixed(igle_xx), igle_yy=fixed(igle_yy), igle_est=fixed(igle_est), igle_pts=fixed(igle_pts), \n",
    "         sel=['Efferent', 'IGLE', 'IMA'])    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3e7ecb-389c-42ff-995e-ef700b6501a4",
   "metadata": {},
   "source": [
    "## **Congratulations**\n",
    "You have successfully completed the second Quilted Tutorial and are now well on your way to becoming a **SPARC** Guru! \n",
    "\n",
    "We invite you to reuse this tutorial and explore the possibilities of using **SPARC** tools when possible or using a different sampling kernel. \n",
    "In particular, the material in this tutorial can be used to generate a stochatic, data-driven neuronal network for simulation. Such simulations cna be run and distirbuted in other sparc tools such as [o<sup>2</sup>S<sup>2</sup>PARC](https://osparc.io/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
